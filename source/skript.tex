\documentclass[11pt,a4paper]{article}
\usepackage{graphicx}
\usepackage[margin=20mm]{geometry}
\usepackage{tabto}
\usepackage{glossaries}

\setlength{\parindent}{0mm}

\makeglossaries

\newglossaryentry{vaddr}
{	
	name={virtual address (vaddr)},
	description={address in process' address space}
}

\newglossaryentry{paddr}
{
	name={physical address (paddr)},
	description={address of real memory}
}

\newglossaryentry{process control block}
{
	name={process control block (PCB)},
	description={Informations about allocated resources of a process}
}

\newglossaryentry{address space}
{
	name={address space (AS)},
	description={Virtrual memory a program can name}}

\begin{document}
	
	\title{OS Zusammenfassung}
	\author{Gr√©goire Mercier}
	
	\maketitle
	
	\begin{abstract}
		This lecture notes are for me to learn LaTeX and review the operating system lecture
	\end{abstract}


	\section{Introduction}
	
	\subsection{Overview}
	
	The Operating System
	
	\begin{itemize}
		\item \textbf{Provides abstraction layer} \newline
		Manages and hides hardware details \newline
		Low-level interfaces to access hardware \newline
		Multiplexes hardware to multiple programs \newline
		makes hardware use effivient for applications
		\item \textbf{Provides protection} from \newline
		users/processes using up all resources \newline
		processes writing into other processes memory \newline
		\item \textbf{is a ressource manager} \newline
		Manages and multiplexes hardware ressources \newline
		Decides between conflicting requests for resource use \newline
		Strives for efficient and fair resource use \newline
		\item \textbf{is a control program} \newline
		Controls execution of programs \newline
		Prevents errors and improper use of the computer
		
	\end{itemize}
	
	There are no universally accepted definitions
	
	\subsection{Hardware}
	
	\textbf{CPU (Central Processing Unit)}
	
	\begin{itemize}
		\item Fetches instructions from memory and executes them
		\item Internal registers store data and metadata during execution
		\item \textbf{User Mode (x86: "Ring 3" or CPL3)} \newline
		Only non-privileged instructions, no hardware managment in this mode for protection
		\item \textbf{Kernel Mode (x86: "Ring 0" or CPL0)} \newline
		All instruction allowed, including privileged instructions \newline
	\end{itemize}
	
	\textbf{RAM (Random Access Memory)} keeps currently execting instructions and data \newline
	
	\textbf{Caching}
	\begin{itemize}
		\item Ram delivers instruction/data slower than the CPU can execute
		\item Memory references typically follow principle of locality
		\item \textbf{Caching} helps mitigating this \textbf{Memory Wall} \newline
		Informations in use are copyed from slower to faster storage. When needed, check whether it is in faster storage before going down in the Memory Hierarchy, then copy it to cache to be used from there
	\end{itemize}
	
	\textbf{Acces times}
	\begin{itemize}
		\item CPU registers					\tab ~1 CPU cycle
		\item L1 cache per core 			\tab ~4 CPU cycles
		\item L2 cache per pair of cores	\tab ~12 CPU cycles
		\item L3 cache						\tab ~28 CPU cycles (~25 GiB/s)
`		\item DDR3-Ram						\tab ~28 CPU cycle for LLC + 50ns (~12 GiB/s)
	\end{itemize}

	\textbf{CPU Cache Organization}
	\begin{itemize}
		\item Caches divided up into cache lines (often 64 bytes each)
		\item Separation of data and instructions in faster caches
		\item \textbf{Cache hit}: Data already in cache
		\item \textbf{Cache miss}: Data has to be fetched from lower level first
		\item Types of Cache misses
		\begin{itemize}
			\item \textbf{Compulsory Miss}: first reference miss, data has never been accessed
			\item \textbf{Capacity Miss}: cache not large enough for Working Set of process
			\item \textbf{Conflict Miss}: cache still has space, but collision due to placement strategy
		\end{itemize}
	\end{itemize}

	\textbf{Device Control}
	
	\begin{itemize}
		\item Device controller accepts command from the OS via device driver
		\item Control by writing into device register and read status by reading it
		\item Data transfer by writing/reading device memory
		\item Port-mapped I/O (PMIO) special CPU instructions to access port-mapped registers and memory
		\item Memory-mapped I/O (MMIO) same address space for Ram and device memory
	\end{itemize}

	\hspace{5mm}Devices can signal the CPU through interrupts
	
	\subsection{OS Invocation}
	
	Operating System Kernel does \textbf{not} always run in the background
	
	Tree occasions invoke the Kernel and switch to kernel-mode
	\begin{itemize}
		\item System calls		\tab User-mode process requires higher privileges
		\item Interrupts		\tab CPU-external device sends a signal
		\item Exceptions		\tab CPU signals an unexpected condition
	\end{itemize}
	
	\textbf{System Calls}
	
	The main Idea behind System calls is the nessecity to protect processes from one another. So processes are running in User-Mode. The OS provides services, which the applications can invoke in System Calls/syscalls, in order to get the action performed by the OS, on behalf of application
	
	Syscall interface between applicateions and OS provides a limited number of well-defined entry points to the kernel
	
	Application Program Interfaces (API) brings another level of abstraction between applications and Programmers (API invokes Syscalls invokes Kernel-Mode operations)
	
	One single entry point to the kernel for all System calls, the \textbf{trap}. Trap switches CPU to kernel mde and enters the kernal in the same, predefined way for every syscall. The system call dispatcher in the kernel acts as a multiplexer for all syscalls.
	
	syscalls identifyed by a number, passed as parameter, \textbf{system call table} maps \textbf{system call number} to kernel funktion, dispatcher decides where to jump based on the number and table.
	
	Programs have the System call number compiled in!
	Never reuse old numbers in future versions of kernel
 	\newline
	
	\textbf{Intrrupts}
	
	Interrupts are used by devices to signal predefined conditions to the OS, they are managed by the Programmable Interrupt Controller. When masked the interrupts are only delivered, when unmasked.
	
	\textbf{interrupt vector}: table pinned in memory containing the adresses of all service routines
	\textbf{interrupt service routine}: takes the control in order to handle a specific interrupt. Saves the state of the interrupted process
	\begin{itemize}
		\item Instruction pointer
		\item Stack pointer
		\item Status word
	\end{itemize}

	
	\textbf{Exceptions}
	\begin{itemize}
		\item Generated by the CPU itself, if an unusual condition makes it impossiible to continue processing
		\item CPU interrupts program and relegate control to the kernel
		\item Kernel determines  the reason for exceptions
		\item If kernel can resolve the problem, it does so and continue the \textbf{faulting instruction}
		\item Otherwise process get killed
	\end{itemize}
	
	Interrupts can happen in \textbf{any} context, Exceptions always occur \textbf{synchronous to} and \textbf{in the context} of a process.
	
	\section{OS Concepts}
	
	Early on, programs were load directly into \textbf{physical memory}. If the program was too large, the  programmer had to manually partition his program into \textbf{overlays}. OS could swap between disk and memory.
	
	Problems: Buggy programs trash other programs, malicious jobs can read other program's operations, Jobs can take all memory for themself,...
	
	\textbf{adress spaces}: every job has his own address space, so they can't reach other jobs adresses. Jobs only use virtual adresses. \newline
	
	\textbf{MMU (memory management unit)}: translates \gls{vaddr} to \gls{paddr}
	
	\begin{itemize}
		\item allows kernel-only virtual addresses
		\item can enforce read-only virtual addresses
		\item can enforce execute disable
	\end{itemize}
	
	Not all addresses need to be mapped at all times. If a virtual address is not mapped, the MMU throws a \textbf{page fault} exception. Handled by loading the faulting address and then continuing the program. \textbf{over-commitment}: more memory than physically aviable. Page faults also issued by MMU on illegal memory access.
	
	A \textbf{process} is a progam in execution, associated with a \gls{process control block} and with a virtual \textbf{address space (AS)}. AS is the only memory a program can name and starts at 0 for every program.
	
	AS are layed out in sections. Memory acces between those sections is illegal and causes a page fault, called \textbf{sementation fault}. Segmentation faults results in the process getting killed by the OS.
	
	A section has the following layout:
	\begin{itemize}
		\item Stack: Function history and local variables
		\item Data: Constatnts, static variables, global variables, strings
		\item Text: Program code
	\end{itemize}
	
	\textbf{Threads} represents execution states of a program
	\begin{itemize}
		\item Instruction pointer (IP) register stores currently executed instruction
		\item Stack pointer (SP) register stores the address of the top of the stack
		\item Program status word (PSW) contains flags about executeion history
		\item ...
	\end{itemize}
	
	Two things to consider when designing an OS:
	
	\textbf{Mechanism}: Implementation of what is done
	
	\textbf{Policy}: The rules which decide when what is done and how much \newline
	
	Operating System need to handle multiple processes and threads in order to provide multi-tasking. The \textbf{scheduler} decides which job to run next, while the \textbf{dispatcher} performs the task-switching. Schedulers provide fairness while trying to reach goals after setted priorities.\newline
	
	Persistent Data is for users is stored in flies and directories. A file is associated with file name and offset with bytes. Directories associate directory names with eigher directory names or file names.
	
	The \textbf{file system} is an ordered collection of blocks, what can be operated on by programmers, with operations like open, read, seek, ...
	
	Processes communicate directly through a special \textbf{named pipe} file
	
	Directories form a \textbf{directory tree/file hierarchy}. The \textbf{root directory} is the topmost directory of a directory tree. Files can be accessed by their \textbf{path name}. \newline
	
	OS abstract the view of information storage to file systems. Drivers hide specific hardware device. OS increases the performance of I/O devices by
	\begin{itemize}
		\item \textbf{Buffering}: Store data temporarily while it is being transferred
		\item \textbf{Caching}: Store parts of data in faster storage for performance
		\item \textbf{Spooling}: Overlap of output of one job with input of other jobs
	\end{itemize}
 	
	\section{Processes}
	
	\subsection{Process Abstraction}
	
	Multiprogramming is the art of switching quickly between processes. Every process is processed in his own "virtual CPU". When switching processes, the execution context changes. On a  \textbf{context switch}, the dispatcher saves the current register and memory mappings and restores those of the next process.
	
	A program is a policy, the process is a mechanism.
	
	With n processes with a process spending p of his time waiting for I/O to complete, then CPU utilization = 1 - ${p^n}$. \newline
	
	\textbf{Concurrency}: Multiple processes on the same CPU
	
	\textbf{Parallelism}: Processes truly rnning at the same time with multiple CPUs
	
	\section{Address Spaces}
	
	Programs can see more memory than aviable (80/20 rule: 80\% of the process memory idle, 20\% active working set). Keep working set in RAM, rest on disk.
	
	\textbf{address space layout}: Organization of code, data and state within process
	Data can be \textbf{fixed sized}, \textbf{free'd in reverse order of allocation} or \textbf{allocated and free'd dynamically}
	
	The \textbf{loader} determines based on an executable file how an executed program is placed in memory \newline
	
	\textbf{Fixed-size Data and Code Segments}
	\begin{itemize}
		\item Data in a program, what has static size, allocated when process created
		\item BSS Segment  (Block Started by Symbol) contains statically-allocated variables and not initialized variabels. The executable file contains the starting address and size of BSS, the entire segment is initially zero
		\item Data segment contains fixed-size, initialized data elements such as global variables
		\item Read-only data segment contains constant numbers and strings
		\item Sometimes BSS, data, and read-only data segment are summarized as a single data segment
	\end{itemize}
	
	\textbf{Stack Segment} \newline
	Data is naturally free'd in reverse order of allocation. Fixed starting point of segment, store top at latest allocation SP (stack pointer). In current CPU, the stack segment typically grows downwards!	\newline
	
	\textbf{Heap Segment}
	Some data needs to be allocated and free'd dynamically "at random", such as input/output, size of edited text, ... \newline
	Allocate memory in two tiers: \newline
	1. Allocate large chunk of memory (heap segment) fom OS, like stack allocation; base address + break pointer (BRK), process can change size by setting BRK \newline
	2. Dynamically partition large chunk into smaller allocation dynamically, with \textit{malloc} and \textit{free}. This happens purely in user space!
	
	\subsection{Typical Process Address Space Layout}
	\begin{itemize}
		\item \textbf{OS}				\tab Adresses where the kernel is mapped 0xFFFFFFFF
		\item \textbf{Stack}		 	\tab Local variables, function call parameters, return addresses
		\item \textbf{Heap}				\tab Dynamically allocated data (malloc)
		\item \textbf{BSS}				\tab Uninitialized local variables dclared as static
		\item \textbf{Data}				\tab Initialized data, global variables
		\item \textbf{RO-Data}		\tab Read-only data, strings
		\item \textbf{Text}				\tab Program, machine code
	\end{itemize}


	\section{Threads}
	In traditional OS, each process has it's own address space, set of allocated resources and one thread of execution. \newline
	Modern OS handle the processes and treads of execution more flexibly. Processe provide the abstraction of an AS and address resources, while threads provide the abstraction for execution state of that AS/container. /par
	Why using multiple Threads? \newline
	So programs can handle many tasks at once. Without threads, some of the tasks could block each other. Many sequential threads are more easy to handle.
	It depents on what is to be done in order to choose between threads and processes. If processes share data, they do it explizitly. Threads allow multiple tasks at once in a single process.
	
	\subsection{Thread Libraries}
	Provide an API for creating and managing threads. Pthreads is the POSIX API for creation and synchronization. It specifies behaviour of the thread library.
	
	Each \textbf{Pthread} is associated with an identifier (Thread ID(TID)), a set of registers (including IP and SP) and a stack area holding the execution state of that thread.
	
	\begin{itemize}
		\item Pthread\_create Create a new thread, passing pointer to pthread\_t (holding TID after successful call), attributes, start funcition and arguments, returning 0 on success or error value.
		\item Pthread\_exit Terminate the calling thread, passing exit code, freeing ressources
		\item Pthread\_join Wait for a specific thread to exit, passing pthread\_t to wait for (or -1 for any thread), pointer to pointer for exit code, returning 0 on success, otherwise error value
		\item Pthread\_yield Release the CPU to let another thread run
	\end{itemize}

	Multithreaded programming is challenging, because there is more shared state than with processes, so more can possibly go wrong. Programmer needs to care about dividing, ordering, and balancing activities, dividing data and synchronize access to shared data.
	
	Processes group resources, threads encapsulate execution. There is a need to differentiate between
	\begin{itemize}
		\item \textbf{Process Control Block (PCB)}: Information needed to implement processes eg. Adress space, open file, child processes, pending alarms. \newline
		The PCB is always known to the OS
		\item \textbf{Thread control Block (TCB)}: Per thread data, eg IP, Registers, Stack, state. Depending on thread model the OS knows about threads or not.
 	\end{itemize}
 
	\subsection{Thread Model Overviev}
	
	OS always knows of at least one thread per process. Threads that are known to the OS are called kernel threads. Threads that are known to the process are called user threads.
	
	\begin{itemize}
		\item \textbf{Many-to-One Model}/Threads fully implemented in user-space: Kernel knows only knows one of possibly multiple threads, user threads are called \textbf{User Level Threads(ULT)}
		\item \textbf{One-to-One Model}/Kernel fully aware of and responsible for managing threads: Each user thread maps to a kernel thread, user threads are called \textbf{Kernel Level Thread(KLT)}
		\item \textbf{M-to-N Model}:Kernel knows some threads per process, but others are known only to the process, flexible mappint of user threads to less kernel threads. Known as hybrid thread model.
	\end{itemize}
	
	
	\textbf{Many-to-One Model: User Level Threads (ULT)} \newline
	The kernel only manages the process, multiple threads are unknown to the kernel. That allows faster thread management operations, a more flexible scheduling policy, fewer system resources and cen be even used when OS does not support threads. But there is no parallel execution possible and if only one thread blocks, the entire process blocks. Also it is needed to reimlement parts of the OS. Linux defines some acitons as followed: \textbf{mkcontext\_t} and \textbf{ucontext\_t} to keep thread state, \textit{makecontext} (initialize a new context), \textit{getcontext} (store currently active context), \textit{setcontext} (replace current context with different one), \textit{swapcontext} (user-level context switching between threads). Periodic threadswitching can be implemented using a \textbf{SIGALRM} exception handler.
	
	Address Space Layout: The "main" part of the \textbf{Stack} is known to OS and is used by thread library. The own execution state for every thread is allocated dynamically on the heap, using \textit{malloc}. There is possibly an own stack for each exception handler. Concurrent \textbf{heap} possible. \newline
	
	\textbf{One-to-One Model: Kernel Threads (KLT)} \newline
	The kernel knows and manages every thread, making real paralellism and individual thread block possible. On the downside, the OS manages every thread in the system, syscalls are needed for thread management and scheduling is fixed in OS.
	
	Address Space Layout: There is an own execution state ($\equiv$\textbf{stack}). Possibly own stack for each exception handler. Parallel \textbf{heap} use is possible, but not all heaps are thread-safe.
	
	Implementation and issues: all thread management data is stored in kernel, management funcions provided as syscalls. Signals are used in UNIX to notify a process that a particular event has occured. The signal handler can run on the process stack, on a stack dedicated to a specific signal handler or a a stack dedicated to all signals. \newline
	
	\textbf{M-to-M Model: Hybrid Threads} \newline
	M ULTs are mapped to (at most) N KLTs, using pros of ULT and KLT: non-blocking with quick management. Provides flexible scheduling policiy and efficient execution, but is hard to implement and to debug.
	
	Implementation: Kernel is not involved in thread activities such as \textit{create} and \textit{join}. Reached by mapping multiple ULTs on each KLT, so when a ULT blocks, the user-space run-time system run a different ULT without switching to the kernel. \textbf{Upcalls}: Kernel notices, that a thread will block and sends a signal to the process. Upcall notifies the process of the thread id and event that happened. Exception handler of the process shedule a different thread in that process. Kernel later informs the process that the blocking event has finished via antother upcall.
	
	\section{Inter Process Communication (IPC)}
	
	Processes and Threads need to communicate with one another frequently. Process cooperate to share information, speed-up computing and provide modularity. IPC allows exchanging data between those processes, buy \textbf{Message passing} (explizitly send and recieve information using system calls) and \textbf{shared memory} (multiple processes using same memory regions). \newline
	
	\subsection{Message Passing}
	Mechanism for process to communicate and synchronize their actions, providing operations to \textit{send} and \textit{recieve}. Implemented by using hardware bus, shared memory, kernel memory and the network interface card. \newline
	
	\textbf{Direct vs. Indirect Messages} \newline
	\textbf{direct messages}: processes name each other explicitly when exchanging, by \textit{send(P, message)} (send a message to process P) and \textit{recieve(Q, message)} (recieve a message form process Q). \newline
	\textbf{indirect messages}: can be sent and recieved from mailoxes. Each mailbox has a unique id. The first communicating process creates mailbox, last destroys mailbox. Process can only communicate if they share a mailbox.. \newline
	
	\textbf{Sender/Reciever Synchronization}
	\begin{itemize}
		\item Message passing may be either \textbf{blocking} or \textbf{non-blocking}
		\item Blocking is considered \textbf{synchronous}
		\item Non-blocking is considered \textbf{asynchronous}
		\item Depending on buffering scheme, non-blocking sender can communicate with non-blocking reciever
	\end{itemize}

	\subsection{Buffering}
	Messages are \textbf{queued} using different capacities while they are in-flight
	\begin{itemize}
		\item Zero capacity - 0 messages/no queuing: Sender must wait for reciever (\textbf{rendezvous}),message is transferred as soon as reciever becomes aviable (no latency/no jitter)
		\item Bounded capacity - finite number and length of messages: Sender can send before reciever waits for messages, sender can send while reciever still processes previous messages, sender musst wait if link full
		\item Unbounded capacity: Sender never waits, memory may overflow, potentially causing very large latency between send and recieve
	\end{itemize}
	
	\subsection{Shared Memory}
	Communicate through a region of shared memory. Processes have shared regions in one another AS. Threads "naturally" share address space. The semantics are application-specific. \newline
	Tricky to get safety and high performance, especially if many processes and many CPUs are involved, due to \textbf{cache coherency protocol}, especially if there are multiple writers, due to {race conditions}. \newline
	
	\textbf{Sequential consistency (SC)} " The result of execution is as if all operations were executed in some sequential order, and the operations of each processor occurred in the order specified by the program", means that all memory operations occure one at a time in program order, ensuring write atomicity. \newline
	CPU and compiler re-order instructions \textbf{execution order} for more efficient execution. Without SC, multiple processes on multiple cores behave "worse" than preemptive threads on a single core. They may give diffeerent results than when interleaving on one core. \newline
	\textbf{Problems:}
	\begin{itemize}
		\item Modern CPUs are generally not sequentially consistent, because it would complicate write buffers, complicate non-blocking reades and make cache coherence more expensive
		\item Compilers don not generate code in program order, they re-arrange loops for better performance, eliminate common subexpressions and cares about software pipelining
		\item As long as a single thread accesses a memory location at a time, this is not a problem
	\end{itemize}
	\textbf{DON'T try to access the same memory location with multiple threads at the same time withouht proper synchroniyation!} \newline
	
	\section{Synchronization}
	\textbf{Race Conditions} (assuming to have sequential memory consistency) \newline
	Occures when two or more non-atomic instruction sequences operates at one time and may end up with a wrong result, because they were not done in the proper sequence. Even operation such as \textit{add count 1} may create race conditions. Only \textbf{interlocked operations} are safe (that implicates that there is only a single interlocked operation for the problem). Interlocked operations are more expensive than regular operations. \newline
	General solution for the \textbf{critical section (CS) problem}: Put non-atomic instruction inside of a critical section. \newline
	\textbf{Desired Properties for Solution to Critical-Section Problem}
	\begin{itemize}
		\item \textbf{Mutual Exclusion}: At most one thread can be in the CS at any time
		\item \textbf{Progress}: No thread running outside of the CS may block another thread from getting in
		\item \textbf{Bounded Waiting}: Once a thread starts trying to enter the CS, there is a bound on the number of times other threads get in
	\end{itemize}
	\textbf{Disabling Interrupts}: While in CS, thread cannot be interrupted, implemented with a "do not interrupt" (DNI) bit. \textit{enter\_critical\_section()} sets DNI bit, \textit{leave\_critical\_section} clears DNI bit, so when interrupts disabled, scheduler is never called. That is easy and convenient in the kernel, but only works in single-core systems, and only feasible in kernel, don't want to give user this power. \newline
	Approach: \textbf{lock variable}: global \textit{lock}, enter CS if lock is 0, set it to 1 when entering, otherwise wait (\textbf{buisy waiting}), but that doesn't solve the CS problem, reading and setting lock is still not atomic. \newline
	Test and set \textit{lock} atomically possible with x86 (\textit{xchg} atomically excaange memory content with a register). Implemented as \textbf{spinlock}. Solves \textbf{mutual exclusion}, \textbf{progress}, but not \textbf{bounded waiting}. Also spinlock doesn't work well
	\begin{itemize}
		\item if the lock is \textbf{congested} (large CS or many threads trying to enter)
		\item if threads on different cores use the lock (expensive to keep memory coherency between cores)
		\item when processes are scheduled with static priorities such as \textbf{priority inversion}, causing unexpected behaviour
	\end{itemize}
	Nevertheless, spinlocks are widely used, especially in kernels \newline
	
	\textbf{Semaphore} \newline
	Idea: busy part of busy waiting is a spinlock limitation. So let threads sleep on locks and wake them up one at a time when lock becomes free. \newline
	Introduce two syscalls, operating on integer variables called \textbf{semaphore}: \textit{wait( \&s )} and \textit{signal( \&s )}. \textit{s} is initialized to maximum number of threads that may enter the CS at any given time. A semaphore initialzed to 1 is called \textbf{binary semaphore}, \textbf{mutex semaphore} or just \textbf{mutex}. \textbf{counting semaphores} allows more than one thread in the CS. \newline
	\textbf{Implementation Considerations}: wait and signal need to be carefully synchronized, otherwise it could result in a race condition between checking and decrementing s. Aditionally, \textbf{signal loss} may occure when waking up threads and waiting at the same time. Each semaphore is associated with a wake-up queue: \textbf{weak semaphores} wake up a random waiting thread, \textbf{strong semaphores} wake up threads in the order in which thez started waiting. \newline
	\textbf{Mutual exclusion}, \textbf{progress} and \textbf{bounded waiting} are solved. But every enter and leave are syscalls, which are slower than regular function calls. \newline
	
	\textbf{Fast User Space Mutex (futex)}
	\begin{itemize}
		\item Userspace and kernel component
		\item Try to get into CS wit a userspace spinlock
		\item If CS buisy, use a syscall to put thread to sleep
		\item Otherwise enter CS lompletly in userspace
	\end{itemize}

	\textbf{Classic synchronization Problems} \newline
	
	\textbf{Producer-Consumer Problem (bounded-buffer problem)}: Buffer shared between a producer and a consumer. \textit{int count} keeps track of number of aviable items. Producer produces items and place them into the buffer, incrementing \textit{count}. If buffer full, producer sleeps until consumer consumed an item. Consumer consumes items, removing them from the buffer and decrementing \textit{count}. If buffer empty, consumer has to sleep until producer produced an item. Solved with a mutex and 2 counting semaphores or condition variables (CV), which allow blocking until a condition is met, with following ideas: New operation that performs unlock, sleep, lock atomically, and new wake-up operation that is called with lock held. \newline
	
	\textbf{Readers-Writers Problem}: Many threads compete to read or write the same data, \textbf{readers} only read data, do not perform any updates, \textbf{writers} can both read and write. It is unnecessary to use a single mutex for read and write, blocking multiple readers while no writer is present. Idea: If no threads writes, multiple readers may be present, if a thread writes, no other readers and writers are allowed
	\begin{itemize}
		\item 1st Readers-Writers Problem: Readers Preference: Writers cannot aquire acces to CS until last reader leaves the section
		\item 2nd Readers-Writers Problem: Writers Preference: No writers should be waiting longer than absolutely necessary
		\item 1st and 2nd readers-writers problem have the same issue: Readers preference -> writers can starve, writers prefernce -> readers can starve
		\item 3rd Readers-Writers Problem: No threads shall starve. Posix treads contains readers-writers lock to address this issue (\textit{pthread\_rwlock}). Multiple readers but only a single writer are let into the CS. If readers are present, while a writer tries to enter the CS, don't let further readers in, block until readers finish, let writer in.
		Really difficult to imlement!
	\end{itemize}
	
	\textbf{Dining-Philosopers Problem} \newline
	Five philosophers are sitting around a table, each one has a plate of spaghetti in front of him, and there is one fork between each one. They can only eat, if they have two forks in their hand. Their cyclic workflow is: 1. Think, 2. Get hungry, 3. Grab one fork, 4. Grab another fork, 5. Eat, 6. Put down forks. No communication allowed, no "atomic" grab of both forks. Problem: What if the all grab their left fork at once. This problem is called \textbf{deadlock}. Workarounds: 4 Philosophers allowed at a table of 5 (\textbf{deadlock avoidance}), odd philosophers take left fork firt, even philosophers take right fork first (\textbf{deadlock prevention}) \newline
	
	\subsection{Deadlocks} 
	Deadlocks can arise if all four conditions hold simultaneously:
	\begin{itemize}
		\item Mutual exclusion (Limited access to resource, resource can only be shared with a finite amount of users)
		\item Hold and wait (wait for next resource while already hold at least one)
		\item No preemption (once the resource is granted, it cannot be taken away but only handed back voluntarily)
		\item Circular wait (possiblity of circularity in graph of requests)
	\end{itemize}
	
	\textbf{Deadlock countermeasures}
	\begin{itemize}
		\item Prevention (pro-active, make deadlocks impossible to occur)
		\item Avoidance (decide on allowed actions based on a-priori knowledge)
		\item Detection (react after deadlock happened/revovery)
	\end{itemize}
	
	\textbf{Deadlock Prevention} \newline
	Negate at least one of the required deadlock conditions:
	\begin{itemize}
		\item Mutual exclusion - buy more resources, split into pieces, virtualize ("infinite" \# of instances)
		\item Hold and wait - get all resources en-block, 2-phase-locking
		\item No preemption - virtualize to make preemptable
		\item Circular waiting - ordering of resources, prevent deadlocks with partial order on resources
	\end{itemize}
	
	\textbf{Deadlock avoidance} \newline
	On every resource request, decide if system stays in saft state, what needs a-priori information. Using Resource Allocation Graph (RAG).\newline
	\textbf{RAG}
	View system state as graph, processes are round nodes, resources are square nodes. Every instance of a resource is depicted as a dot in the resource node.\newline
	Resource requests and assignmeents are edges:
	\begin{itemize}
		\item Resource pointing to process: Resource is assigned to process
		\item Process pointing to resource: Process is requesting resource
		\item Process may request resoure: Claim edge, depicted as dotted line
	\end{itemize}
	
	\textbf{Deadlock Detection}
	Allow system to enter deadlock, detect it, apply recovery scheme. \newline
	Maintain \textbf{Wait-For Graph(WFG)}. Periodically invoce an algorithm that searches for a cycle in the graph, if there is a cycle, there exists a deadlock.\newline
	\textbf{Recovery}: Abort all deadlocked processes/Abort one process at a time until the deadlock cycle is eliminated. \newline
	
	
	\subsection{Implementation}
	Synchronisation problems occur very often when programming operating systems, the parallelism introduced by multiple processors and the concurrency introduced by multiprogramming needs to be considered carefully when writing an operating system, poorly synchronized code can lead to starvation, priority inversion or deadlocks. \newline
	
	
	
	
	
	\newpage
	
	\printglossaries
	
	
	
\end{document}